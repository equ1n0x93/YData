{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inbarhub/YDATA_DL_assignments_2021-2022/blob/main/H.W_9_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88X5OhoAJEJh"
      },
      "source": [
        "# RNN for text generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh_aPOY6JEJz"
      },
      "source": [
        "In this exercise, you'll unleash the hidden creativity of your computer, by letting it generate Country songs (yeehaw!). You'll train a character-level RNN-based language model, and use it to generate new songs.\n",
        "\n",
        "\n",
        "### Special Note\n",
        "\n",
        "Our Deep Learning course was packed with both theory and practice. In a short time, you've got to learn the basics of deep learning theory and get hands-on experience training and using pretrained DL networks, while learning PyTorch.  \n",
        "Past exercises required a lot of work, and hopefully gave you a sense of the challenges and difficulties one faces when using deep learning in the real world. While the investment you've made in the course so far is enormous, We strongly encourage you to take a stab at this exercise. \n",
        "\n",
        "Some songs contain no lyrics (for example, they just contain the text \"instrumental\"). Others include non-English characters. You'll often need to preprocess your data and make decisions as to what your network should actually get as input (think - how should you treat newline characters?)\n",
        "\n",
        "More issues will probably pop up while you're working on this task. If you face technical difficulties or find a step in the process that takes too long, please let me know. It would also be great if you share with the class code you wrote that speeds up some of the work (for example, a data loader class, a parsed dataset etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npjwCeQRJEJ_"
      },
      "source": [
        "## RNN for Text Generation\n",
        "In this section, we'll use an LSTM to generate new songs. You can pick any genre you like, or just use all genres. You can even try to generate songs in the style of a certain artist - remember that the Metrolyrics dataset contains the author of each song. \n",
        "\n",
        "For this, we’ll first train a character-based language model. We’ve mostly discussed in class the usage of RNNs to predict the next word given past words, but as we’ve mentioned in class, RNNs can also be used to learn sequences of characters.\n",
        "\n",
        "First, please go through the [PyTorch tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html) on generating family names. You can download a .py file or a jupyter notebook with the entire code of the tutorial. \n",
        "\n",
        "As a reminder of topics we've discussed in class, see Andrej Karpathy's popular blog post [\"The Unreasonable Effectiveness of Recurrent Neural Networks\"](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). You are also encouraged to view [this](https://gist.github.com/karpathy/d4dee566867f8291f086) vanilla implementation of a character-level RNN, written in numpy with just 100 lines of code, including the forward and backward passes.  \n",
        "\n",
        "Other tutorials that might prove useful:\n",
        "1. http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/\n",
        "1. https://github.com/mcleonard/pytorch-charRNN\n",
        "1. https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gfQSvdwVQoZ"
      },
      "source": [
        "### Final Tips\n",
        "As a final tip, we do encourage you to do most of the work first on your local machine. They say that Data Scientists spend 80% of their time cleaning the data and preparing it for training (and 20% complaining about cleaning the data and preparing it). Handling these parts on your local machine usually mean you will spend less time complaining. You can switch to the cloud once your code runs and your pipeline is in place, for the actual training using a GPU.  \n",
        "\n",
        "We also encourage you to use a small subset of the dataset first, so things run smoothly. The Metrolyrics dataset contains over 300k songs. You can start with a much much smaller set (even 3,000 songs) and try to train a network based on it. Once everything runs properly, add more data. \n",
        "\n",
        "Good luck!  \n",
        "\n",
        "---\n",
        "#### This exericse was originally written by Dr. Omri Allouche."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 24469,
          "status": "ok",
          "timestamp": 1652257692821,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "bwO0SnykskFg",
        "outputId": "f8ddf68b-888a-4f3c-a857-c00eaf120d54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Same imports from the previous songs task\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "executionInfo": {
          "elapsed": 3135,
          "status": "ok",
          "timestamp": 1652257695952,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "Jtr4OlJlUdd-"
      },
      "outputs": [],
      "source": [
        "# Some imports for the notebook to work\n",
        "import re\n",
        "import time\n",
        "import copy\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sklearn imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeF1-RyXtbUL"
      },
      "source": [
        "### Read the data and clean it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 8848,
          "status": "ok",
          "timestamp": 1652257704797,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "I9epyeKDtYu1"
      },
      "outputs": [],
      "source": [
        "raw_df = pd.read_parquet(\n",
        "    \"/content/drive/MyDrive/Y-Data/Semester II/Deep Learning/Assignments/Assignment 8 - Word Embedding + Text Classification/data/metrolyrics.parquet\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "executionInfo": {
          "elapsed": 15,
          "status": "ok",
          "timestamp": 1652257704797,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "VGu71jqCJEKE",
        "outputId": "86361a27-ec14-4c55-b91d-2ee5ed7846b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[HEALY]\n",
            "[spoken] This is Bert Healy saying ...\n",
            "[singing now] Hey, hobo man\n",
            "Hey, Dapper Dan\n",
            "You've both got your style\n",
            "But Brother,\n",
            "You're never fully dressed\n",
            "Without a smile!\n",
            "Your clothes may be Beau Brummelly\n",
            "They stand out a mile --\n",
            "But Brother,\n",
            "You're never fully dressed\n",
            "Without a smile!\n",
            "Who cares what they're wearing\n",
            "On Main Street,\n",
            "Or Saville Row,\n",
            "It's what you wear from ear to ear\n",
            "And not from head to toe\n",
            "(That matters)\n",
            "So, Senator,\n",
            "So, Janitor,\n",
            "So long for a while\n",
            "Remember,\n",
            "You're never fully dressed\n",
            "Without a smile!\n",
            "[BOYLAN SISTER]\n",
            "Ready or not, here he goes\n",
            "Listen to Bert\n",
            "Tap his smilin' toes\n",
            "[HEALY]\n",
            "[spoken] Ah, the lovely Boylan Sisters\n",
            "[BOYLAN SISTERS]\n",
            "Doo doodle-oo doo\n",
            "Doo doodle-oo doo\n",
            "Doo doo doo doo\n",
            "Doo doo doo doo\n",
            "Your clothes may be Beau Brummelly\n",
            "They stand out a mile\n",
            "But, bother\n",
            "You're never fully dressed\n",
            "You're never dressed\n",
            "Without an\n",
            "[CONNIE BOYLAN]\n",
            "S-\n",
            "[BONNIE BOYLAN]\n",
            "M-\n",
            "[RONNIE BOYLAN]\n",
            "I-\n",
            "[CONNIE BOYLAN]\n",
            "L-\n",
            "[ALL THREE]\n",
            "E.\n",
            "Smile darn ya smile.\n",
            "[ALL]\n",
            "That matters\n",
            "So Senator\n",
            "So Janitor\n",
            "So long for a while\n",
            "healy spoken this bert healy saying singing hey hobo man hey dapper dan you got style but brother you never fully dressed without smile your clothes may beau brummelly they stand mile but brother you never fully dressed without smile who cares wearing on main street or saville row it wear ear ear and head toe that matters so senator so janitor so long remember you never fully dressed without smile boylan sister ready goes listen bert tap smilin toes healy spoken ah lovely boylan sisters boylan sisters doo doodle oo doo doo doodle oo doo doo doo doo doo doo doo doo doo your clothes may beau brummelly they stand mile but bother you never fully dressed you never dressed without connie boylan s bonnie boylan m ronnie boylan i connie boylan l all three e smile darn ya smile all that matters so senator so janitor so long\n"
          ]
        }
      ],
      "source": [
        "print(raw_df.iloc[0].lyrics)\n",
        "print(raw_df.iloc[0].sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "executionInfo": {
          "elapsed": 14,
          "status": "ok",
          "timestamp": 1652257704798,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "LRGG7jb2Ouye",
        "outputId": "27eb489d-b56a-448d-a7dd-f182440ab266"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "string.printable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 8677,
          "status": "ok",
          "timestamp": 1652257933683,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "QSqh-GaWtrj4",
        "outputId": "372719db-1669-40b3-df56-cb0ac70ae518"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "song                                                       fully-dressed\n",
              "year                                                                2008\n",
              "artist                                                             annie\n",
              "genre                                                                Pop\n",
              "lyrics                 [HEALY]\\n[spoken] This is Bert Healy saying .....\n",
              "num_chars                                                           1041\n",
              "sent                   healy spoken this bert healy saying singing he...\n",
              "num_words                                                            826\n",
              "lyrics_preprocessed     \\n  This is Bert Healy saying ...\\n  Hey, hob...\n",
              "Name: 204182, dtype: object"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_characters = string.printable\n",
        "number_of_characters = len(all_characters)\n",
        "\n",
        "\n",
        "def preprocess_words(raw_str):\n",
        "    # 1. Remove problematic seen regex matches\n",
        "    cleaned_str = re.sub(\n",
        "        \"(\\[[a-zA-Z ,-:!]+\\]|\\([a-zA-Z \\*,-:!]+\\)|\\{[a-zA-Z \\*,-:!]+\\}|--[a-zA-Z \\*,-:!]+--|chorus)\",\n",
        "        \" \",\n",
        "        raw_str,\n",
        "    )\n",
        "\n",
        "    # 2. Remove characters out of our all_characters dictionary\n",
        "    final_str = \"\".join(filter(lambda x: x in string.printable, cleaned_str))\n",
        "    return final_str\n",
        "\n",
        "\n",
        "def preprocess_df(df):\n",
        "    # Remove songs without lyrics\n",
        "    df = df[~df.lyrics.str.contains(\"licensing restrictions\")]\n",
        "\n",
        "    # Remove songs with non-english characters\n",
        "    df = df[df.lyrics.map(lambda x: x.isascii())]\n",
        "\n",
        "    df[\"lyrics_preprocessed\"] = df.lyrics.map(lambda x: preprocess_words(x))\n",
        "\n",
        "    # Limit lyrics to min of 100 chars and max length of 3000 (avoid short instrumental songs / too long inputs)\n",
        "    df = df[\n",
        "        (df.lyrics_preprocessed.str.len() >= 100)\n",
        "        & (df.lyrics_preprocessed.str.len() <= 3000)\n",
        "    ]\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "pp_df = preprocess_df(raw_df)\n",
        "pp_df.iloc[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cig0atf1BAlT"
      },
      "source": [
        "### Review the lyrics and evaluate different problems in the input\n",
        "\n",
        "Some of the issues seen when looking manually on songs:\n",
        "\n",
        "**Strings not related to lyrics wrapped with ( or [ or { or --**\n",
        "\n",
        "1. [ steel - trumpet ]\n",
        "2. [guitar]\n",
        "3. (One more time) \n",
        "4. (repeat chorus)\n",
        "5. (Dolly Parton)\n",
        "6. --- Piano - Guitar Instrumental ---\n",
        "\n",
        "**Non english (no ascii chars)**\n",
        "\n",
        "1. Avec le temps tout s'√É¬©vanouit\n",
        "\n",
        "**Credits**\n",
        "SHADOWS OF HER MIND\n",
        "(Kris Kristofferson)\n",
        "'68 Careers BMG Music\n",
        "\n",
        "**Chorus repeats in many forms**\n",
        "\n",
        "CHORUS:\n",
        "1. [ Chorus ]\n",
        "2. (Chorus)\n",
        "\n",
        "**Instrumental / No lyrics**\n",
        "\n",
        "1. Katy Hill is a traditional song instrumental. There are no known lyrics. Popularized by Bill Monroe playing with Jerry Garcia (Greatful Dead fame).\n",
        "\n",
        "2. We are not in a position to display these lyrics due to licensing restrictions. Sorry for the inconvinience.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "executionInfo": {
          "elapsed": 17,
          "status": "ok",
          "timestamp": 1652257933684,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "jlwXzpoh9UXg",
        "outputId": "56ea14f4-491b-4916-a68e-0fc1233e8aad"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" \\n  This is Bert Healy saying ...\\n  Hey, hobo man\\nHey, Dapper Dan\\nYou've both got your style\\nBut Brother,\\nYou're never fully dressed\\nWithout a smile!\\nYour clothes may be Beau Brummelly\\nThey stand out a mile --\\nBut Brother,\\nYou're never fully dressed\\nWithout a smile!\\nWho cares what they're wearing\\nOn Main Street,\\nOr Saville Row,\\nIt's what you wear from ear to ear\\nAnd not from head to toe\\n \\nSo, Senator,\\nSo, Janitor,\\nSo long for a while\\nRemember,\\nYou're never fully dressed\\nWithout a smile!\\n \\nReady or not, here he goes\\nListen to Bert\\nTap his smilin' toes\\n \\n  Ah, the lovely Boylan Sisters\\n \\nDoo doodle-oo doo\\nDoo doodle-oo doo\\nDoo doo doo doo\\nDoo doo doo doo\\nYour clothes may be Beau Brummelly\\nThey stand out a mile\\nBut, bother\\nYou're never fully dressed\\nYou're never dressed\\nWithout an\\n \\nS-\\n \\nM-\\n \\nI-\\n \\nL-\\n \\nE.\\nSmile darn ya smile.\\n \\nThat matters\\nSo Senator\\nSo Janitor\\nSo long for a while\""
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "re.sub(\n",
        "    \"(\\[[a-zA-Z ,-:!]+\\]|\\([a-zA-Z \\*,-:!]+\\)|\\{[a-zA-Z \\*,-:!]+\\}|--[a-zA-Z \\*,-:!]+--|chorus)\",\n",
        "    \" \",\n",
        "    raw_df.iloc[0].lyrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "executionInfo": {
          "elapsed": 15,
          "status": "ok",
          "timestamp": 1652257933684,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "fOvoVsjI9d0e",
        "outputId": "a14f5a25-7861-4fb0-fe4f-d888586ec479"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"[HEALY]\\n[spoken] This is Bert Healy saying ...\\n[singing now] Hey, hobo man\\nHey, Dapper Dan\\nYou've both got your style\\nBut Brother,\\nYou're never fully dressed\\nWithout a smile!\\nYour clothes may be Beau Brummelly\\nThey stand out a mile --\\nBut Brother,\\nYou're never fully dressed\\nWithout a smile!\\nWho cares what they're wearing\\nOn Main Street,\\nOr Saville Row,\\nIt's what you wear from ear to ear\\nAnd not from head to toe\\n(That matters)\\nSo, Senator,\\nSo, Janitor,\\nSo long for a while\\nRemember,\\nYou're never fully dressed\\nWithout a smile!\\n[BOYLAN SISTER]\\nReady or not, here he goes\\nListen to Bert\\nTap his smilin' toes\\n[HEALY]\\n[spoken] Ah, the lovely Boylan Sisters\\n[BOYLAN SISTERS]\\nDoo doodle-oo doo\\nDoo doodle-oo doo\\nDoo doo doo doo\\nDoo doo doo doo\\nYour clothes may be Beau Brummelly\\nThey stand out a mile\\nBut, bother\\nYou're never fully dressed\\nYou're never dressed\\nWithout an\\n[CONNIE BOYLAN]\\nS-\\n[BONNIE BOYLAN]\\nM-\\n[RONNIE BOYLAN]\\nI-\\n[CONNIE BOYLAN]\\nL-\\n[ALL THREE]\\nE.\\nSmile darn ya smile.\\n[ALL]\\nThat matters\\nSo Senator\\nSo Janitor\\nSo long for a while\""
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_df.iloc[0].lyrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1652257934120,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "XWJnY6Wwtwdq",
        "outputId": "3c0a4407-a3a7-46d4-c8b9-b7fe2bae9ec8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'{*the first 1:40 of this song is instrumental*}\\n[Kool Keith]\\nBreak it down..\\n{*another 0:25 of instrumental*}\\n{*scratched: \"\"This is the year to make money and be chillin..\"\" 2X*}\\n[Kool Keith]\\nBreak it down..\\n\"\"This.. is.. the.. year.. to make.. money..\\nand.. be ch-ch-chillin..\"\"\\n\"\"This is the year to make money and be chillin..\"\"\\n[Kool Keith]\\nSort of.. break it.. down like this\\n{*another 0:25 of instrumental*}\\n[Kool Keith]\\nYes.. as we pass through the interludes..\\nThe J-A-Z-Z session, continues!\\nOne two, rock.. lyrics..\\nI get rough make raps collide rough; they stick together\\nlike Run-D.M.C. pumpin up, \"\"Tougher Than Leather\"\"\\nVertical sideways earthquakes\\'ll shake them highways\\nTechnics spin rotate, the red lights illustrate\\nLike groupies on Shaquille, so swift like Ron O\\'Neal\\nwhile Freddie remains Dead with speakers pumpin in his head\\nMore wild than GoreFest - GRRRRRRRRRRROWWWWL,'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pp_df[pp_df.lyrics.str.contains(\"instrumental\")].iloc[-4].lyrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1652257934561,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "OnBEkR-i-hGX"
      },
      "outputs": [],
      "source": [
        "country_df = pp_df[pp_df.genre == \"Country\"].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1652257935869,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "68XIlzRD-B1g",
        "outputId": "5b665bf7-c051-4ff0-e3cc-5641a1e4561f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "58928     we have seen the star\\nand have come to worshi...\n",
              "245636    I dreamed of my dad and my mama, and baby when...\n",
              "Name: lyrics, dtype: object"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "country_df[country_df.lyrics.str.len() < 110].lyrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "executionInfo": {
          "elapsed": 5,
          "status": "ok",
          "timestamp": 1652257936787,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "q5v7jlJc-5O3",
        "outputId": "c04ac449-834e-4a43-9be3-0a4a5960f1ad"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'we have seen the star\\nand have come to worship him\\nwe have seen the star\\nand have come to worship him'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "country_df[country_df.lyrics.str.len() < 110].lyrics.iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1652257937234,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "I2sACs4iGs_b",
        "outputId": "528b1dda-1d00-4313-a557-63f447f52313"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8b820507-fa5a-4250-91ad-624ee7ba3283\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>song</th>\n",
              "      <th>year</th>\n",
              "      <th>artist</th>\n",
              "      <th>genre</th>\n",
              "      <th>lyrics</th>\n",
              "      <th>num_chars</th>\n",
              "      <th>sent</th>\n",
              "      <th>num_words</th>\n",
              "      <th>lyrics_preprocessed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>127800</th>\n",
              "      <td>the-last-goodbye</td>\n",
              "      <td>2008</td>\n",
              "      <td>aaron-pritchett</td>\n",
              "      <td>Country</td>\n",
              "      <td>Sprintime in Savannah\\nIt dont get much pretti...</td>\n",
              "      <td>881</td>\n",
              "      <td>sprintime savannah it dont get much prettier b...</td>\n",
              "      <td>639</td>\n",
              "      <td>Sprintime in Savannah\\nIt dont get much pretti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>963</th>\n",
              "      <td>wherever-is-your-heart</td>\n",
              "      <td>2015</td>\n",
              "      <td>brandi-carlile</td>\n",
              "      <td>Country</td>\n",
              "      <td>I think it's time we found a way back home\\nYo...</td>\n",
              "      <td>1635</td>\n",
              "      <td>i think time found way back home you loose man...</td>\n",
              "      <td>1196</td>\n",
              "      <td>I think it's time we found a way back home\\nYo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>216996</th>\n",
              "      <td>tree-of-hearts</td>\n",
              "      <td>2006</td>\n",
              "      <td>bryan-white</td>\n",
              "      <td>Country</td>\n",
              "      <td>On a tree inside a heart\\nA dull boyscout knif...</td>\n",
              "      <td>1193</td>\n",
              "      <td>on tree inside heart a dull boyscout knife too...</td>\n",
              "      <td>873</td>\n",
              "      <td>On a tree inside a heart\\nA dull boyscout knif...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159985</th>\n",
              "      <td>fireworks</td>\n",
              "      <td>2008</td>\n",
              "      <td>caitlin-cary</td>\n",
              "      <td>Country</td>\n",
              "      <td>Oh, when you're leaving for the hundredth time...</td>\n",
              "      <td>1051</td>\n",
              "      <td>oh leaving hundredth time day looking reason r...</td>\n",
              "      <td>664</td>\n",
              "      <td>Oh, when you're leaving for the hundredth time...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133958</th>\n",
              "      <td>i-can-get-by</td>\n",
              "      <td>2007</td>\n",
              "      <td>clint-black</td>\n",
              "      <td>Country</td>\n",
              "      <td>In a simple life dreams die hard\\nYou never le...</td>\n",
              "      <td>1437</td>\n",
              "      <td>in simple life dreams die hard you never let e...</td>\n",
              "      <td>907</td>\n",
              "      <td>In a simple life dreams die hard\\nYou never le...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8b820507-fa5a-4250-91ad-624ee7ba3283')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8b820507-fa5a-4250-91ad-624ee7ba3283 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8b820507-fa5a-4250-91ad-624ee7ba3283');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                          song  year           artist    genre  \\\n",
              "127800        the-last-goodbye  2008  aaron-pritchett  Country   \n",
              "963     wherever-is-your-heart  2015   brandi-carlile  Country   \n",
              "216996          tree-of-hearts  2006      bryan-white  Country   \n",
              "159985               fireworks  2008     caitlin-cary  Country   \n",
              "133958            i-can-get-by  2007      clint-black  Country   \n",
              "\n",
              "                                                   lyrics  num_chars  \\\n",
              "127800  Sprintime in Savannah\\nIt dont get much pretti...        881   \n",
              "963     I think it's time we found a way back home\\nYo...       1635   \n",
              "216996  On a tree inside a heart\\nA dull boyscout knif...       1193   \n",
              "159985  Oh, when you're leaving for the hundredth time...       1051   \n",
              "133958  In a simple life dreams die hard\\nYou never le...       1437   \n",
              "\n",
              "                                                     sent  num_words  \\\n",
              "127800  sprintime savannah it dont get much prettier b...        639   \n",
              "963     i think time found way back home you loose man...       1196   \n",
              "216996  on tree inside heart a dull boyscout knife too...        873   \n",
              "159985  oh leaving hundredth time day looking reason r...        664   \n",
              "133958  in simple life dreams die hard you never let e...        907   \n",
              "\n",
              "                                      lyrics_preprocessed  \n",
              "127800  Sprintime in Savannah\\nIt dont get much pretti...  \n",
              "963     I think it's time we found a way back home\\nYo...  \n",
              "216996  On a tree inside a heart\\nA dull boyscout knif...  \n",
              "159985  Oh, when you're leaving for the hundredth time...  \n",
              "133958  In a simple life dreams die hard\\nYou never le...  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "country_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1652257937792,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "jdkFqF882-sO"
      },
      "outputs": [],
      "source": [
        "country_df.lyrics.to_csv(\"./lyrics_export.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhjDVAJwGIog"
      },
      "source": [
        "### Create the PyTorch datasets and train the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1652257939157,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "C4igqmn9HNdj",
        "outputId": "f3b418a9-e7f7-4b35-f3ac-993d8931036d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1652257940063,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "lK7yaYJrGHaW"
      },
      "outputs": [],
      "source": [
        "def character_to_label(character):\n",
        "    \"\"\"Returns a one-hot-encoded tensor given a character.\n",
        "\n",
        "    Uses string.printable as a dictionary.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    character : str\n",
        "        A character\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    one_hot_tensor : Tensor of shape (1, number_of_characters)\n",
        "        One-hot-encoded tensor\n",
        "    \"\"\"\n",
        "\n",
        "    character_label = all_characters.find(character)\n",
        "\n",
        "    return character_label\n",
        "\n",
        "\n",
        "def string_to_labels(character_string):\n",
        "    \"\"\"Returns an array of incides per character in the given string\"\"\"\n",
        "    return [character_to_label(c) for c in character_string]\n",
        "\n",
        "\n",
        "def pad_sequence(seq, max_length, pad_label=100):\n",
        "    \"\"\"Pads the sequence to the max length\"\"\"\n",
        "    seq += [pad_label for i in range(max_length - len(seq))]\n",
        "\n",
        "    return seq\n",
        "\n",
        "\n",
        "class LyricsGenerationDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, lyrics_df, max_lyrics_len, minimum_song_count=None, artists=None\n",
        "    ):\n",
        "        # Copy the original dataframe\n",
        "        self.lyrics_dataframe = lyrics_df.copy()\n",
        "\n",
        "        # Can ignore\n",
        "        if artists:\n",
        "            self.lyrics_dataframe = self.lyrics_dataframe[\n",
        "                self.lyrics_dataframe.artist.isin(artists)\n",
        "            ]\n",
        "            self.lyrics_dataframe = self.lyrics_dataframe.reset_index()\n",
        "\n",
        "        # Can ignore as well (relevant when looking per artist)\n",
        "        if minimum_song_count:\n",
        "            # Getting artists that have 70+ songs\n",
        "            self.lyrics_dataframe = self.lyrics_dataframe.groupby(\"artist\").filter(\n",
        "                lambda x: len(x) > minimum_song_count\n",
        "            )\n",
        "            # Reindex .loc after we fetched random songs\n",
        "            self.lyrics_dataframe = self.lyrics_dataframe.reset_index()\n",
        "\n",
        "        # Get the length of the biggest lyric text\n",
        "        # We will need that for padding\n",
        "        self.max_lyrics_len = max_lyrics_len\n",
        "\n",
        "        self.indexes = list(range(len(self.lyrics_dataframe)))\n",
        "\n",
        "        self.artists_list = list(self.lyrics_dataframe.artist.unique())\n",
        "\n",
        "        self.number_of_artists = len(self.artists_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indexes)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = self.indexes[index]\n",
        "\n",
        "        # Return the string of lyrics\n",
        "        sequence_raw_string = self.lyrics_dataframe.iloc[index].lyrics_preprocessed\n",
        "        # Convert the string to characters indices array\n",
        "        sequence_string_labels = string_to_labels(sequence_raw_string)\n",
        "        # Get the current sequence max index (based on length)\n",
        "        sequence_length = len(sequence_string_labels) - 1\n",
        "\n",
        "        # Shifted by one char\n",
        "        input_string_labels = sequence_string_labels[:-1]\n",
        "        output_string_labels = sequence_string_labels[1:]\n",
        "\n",
        "        # pad sequence so that all of them have the same length\n",
        "        # Otherwise the batching won't work\n",
        "        input_string_labels_padded = pad_sequence(\n",
        "            input_string_labels, max_length=self.max_lyrics_len\n",
        "        )\n",
        "\n",
        "        output_string_labels_padded = pad_sequence(\n",
        "            output_string_labels, max_length=self.max_lyrics_len, pad_label=-100\n",
        "        )\n",
        "\n",
        "        return (\n",
        "            torch.LongTensor(input_string_labels_padded),\n",
        "            torch.LongTensor(output_string_labels_padded),\n",
        "            torch.LongTensor([sequence_length]),\n",
        "        )\n",
        "\n",
        "\n",
        "def post_process_sequence_batch(batch_tuple):\n",
        "    # Break tuple\n",
        "    input_sequences, output_sequences, lengths = batch_tuple\n",
        "\n",
        "    # split to tuple of matrix rows each row is lyrics\n",
        "    splitted_input_sequence_batch = input_sequences.split(split_size=1)\n",
        "    splitted_output_sequence_batch = output_sequences.split(split_size=1)\n",
        "    splitted_lengths_batch = lengths.split(split_size=1)\n",
        "\n",
        "    # zip to tuples of input line, output line, length\n",
        "    training_data_tuples = zip(\n",
        "        splitted_input_sequence_batch,\n",
        "        splitted_output_sequence_batch,\n",
        "        splitted_lengths_batch,\n",
        "    )\n",
        "\n",
        "    training_data_tuples_sorted = sorted(\n",
        "        training_data_tuples, key=lambda p: int(p[2]), reverse=True\n",
        "    )\n",
        "\n",
        "    (\n",
        "        splitted_input_sequence_batch,\n",
        "        splitted_output_sequence_batch,\n",
        "        splitted_lengths_batch,\n",
        "    ) = zip(*training_data_tuples_sorted)\n",
        "\n",
        "    # reduce dim to get matrices\n",
        "    input_sequence_batch_sorted = torch.cat(splitted_input_sequence_batch)\n",
        "    output_sequence_batch_sorted = torch.cat(splitted_output_sequence_batch)\n",
        "    lengths_batch_sorted = torch.cat(splitted_lengths_batch)\n",
        "\n",
        "    # concat the padding area to be from the same len of the longest lyrics\n",
        "    input_sequence_batch_sorted = input_sequence_batch_sorted[\n",
        "        :, : lengths_batch_sorted[0, 0]\n",
        "    ]\n",
        "    output_sequence_batch_sorted = output_sequence_batch_sorted[\n",
        "        :, : lengths_batch_sorted[0, 0]\n",
        "    ]\n",
        "\n",
        "    # switch rows to columns, each row is lyrics\n",
        "    input_sequence_batch_transposed = input_sequence_batch_sorted.transpose(0, 1)\n",
        "\n",
        "    # pytorch's api for rnns wants lenghts to be list of ints\n",
        "    lengths_batch_sorted_list = list(lengths_batch_sorted)\n",
        "    lengths_batch_sorted_list = [int(x) for x in lengths_batch_sorted_list]\n",
        "\n",
        "    return (\n",
        "        input_sequence_batch_transposed,\n",
        "        output_sequence_batch_sorted,\n",
        "        lengths_batch_sorted_list,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1652257956444,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "vWXIdkxZGPsz"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes, n_layers=2):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_classes = num_classes\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Converts labels into one-hot encoding and runs a linear\n",
        "        # layer on each of the converted one-hot encoded elements\n",
        "\n",
        "        # input_size -- size of the dictionary + 1 (accounts for padding constant)\n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "\n",
        "        self.gru = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
        "\n",
        "        self.logits_fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_sequences, input_sequences_lengths, hidden=None):\n",
        "        embedded = self.encoder(input_sequences)\n",
        "\n",
        "        # Here we run rnns only on non-padded regions of the batch\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(\n",
        "            embedded, input_sequences_lengths\n",
        "        )\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(\n",
        "            outputs\n",
        "        )  # unpack (back to padded)\n",
        "\n",
        "        logits = self.logits_fc(outputs)\n",
        "\n",
        "        logits = logits.transpose(0, 1).contiguous()\n",
        "\n",
        "        logits_flatten = logits.view(-1, self.num_classes)\n",
        "\n",
        "        return logits_flatten, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xjIUyu4jNhb"
      },
      "source": [
        "Let's train on a subset of the dataset first and see that we can get the network to learn anything before the heavy training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1652257956887,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "RScTh7iFUSRx",
        "outputId": "dba61cde-5141-4187-b64c-3bfa1ba6c592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset_sizes:  {'train': 800, 'val': 200}\n"
          ]
        }
      ],
      "source": [
        "sample_ds = country_df.head(1000)\n",
        "max_lyrics_len = sample_ds.lyrics_preprocessed.str.len().max()\n",
        "\n",
        "ds_train, ds_test = train_test_split(\n",
        "    sample_ds, test_size=0.2, random_state=42\n",
        ")  # 80% / 20% split\n",
        "\n",
        "trainset_sample = LyricsGenerationDataset(ds_train, max_lyrics_len=max_lyrics_len)\n",
        "testset_sample = LyricsGenerationDataset(ds_test, max_lyrics_len=max_lyrics_len)\n",
        "\n",
        "dataloaders = {\n",
        "    \"train\": torch.utils.data.DataLoader(\n",
        "        trainset_sample, batch_size=32, shuffle=True, num_workers=2\n",
        "    ),\n",
        "    \"val\": torch.utils.data.DataLoader(\n",
        "        testset_sample, batch_size=32, shuffle=False, num_workers=2\n",
        "    ),\n",
        "}\n",
        "\n",
        "dataset_sizes = {x: len(dataloaders[x].dataset) for x in [\"train\", \"val\"]}\n",
        "print(\"dataset_sizes: \", dataset_sizes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 476,
          "status": "ok",
          "timestamp": 1652257957801,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "mfkkKikWXW71",
        "outputId": "c5113936-54ea-4586-ca42-b1123311b89c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([32, 2729]), torch.Size([32, 2729]), torch.Size([32, 1]))"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get a batch of training data - the data loader is a generator\n",
        "inputs, outputs, seq_length = next(iter(dataloaders[\"train\"]))\n",
        "inputs.shape, outputs.shape, seq_length.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1652257959212,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "g9oMkXyeXJ7r"
      },
      "outputs": [],
      "source": [
        "# Training code - Pretty much the same from the previous tasks\n",
        "\n",
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    # Init variables that will save info about the best model\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = np.inf\n",
        "    results = {\"train_loss\": [], \"test_loss\": []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"Epoch {}/{}\".format(epoch, num_epochs - 1))\n",
        "        print(\"-\" * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in [\"train\", \"val\"]:\n",
        "            if phase == \"train\":\n",
        "                # Set model to training mode.\n",
        "                model.train()\n",
        "            else:\n",
        "                # Set model to evaluate mode. In evaluate mode, we don't perform backprop and don't need to keep the gradients\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "\n",
        "            # Iterate over data\n",
        "            # for inputs, labels in dataloaders[phase]:\n",
        "            for batch in dataloaders[phase]:\n",
        "                # print(f\"Pre-process shapes: {batch[0].shape}, {batch[1].shape}, {batch[2]}\")\n",
        "                post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
        "                input_sequences_batch, output_sequences_batch, sequences_lengths = (\n",
        "                    post_processed_batch_tuple\n",
        "                )\n",
        "                # print(f\"Post-process shapes: {input_sequences_batch.shape}, {output_sequences_batch.shape}, {sequences_lengths}\")\n",
        "                # Prepare the inputs for GPU/CPU\n",
        "                inputs = input_sequences_batch.long().to(device)\n",
        "                outputs = output_sequences_batch.contiguous().view(-1).long().to(device)\n",
        "                # labels = labels.to(device)\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # ===== forward pass ======\n",
        "                with torch.set_grad_enabled(phase == \"train\"):\n",
        "                    # If we're in train mode, we'll track the gradients to allow back-propagation\n",
        "                    logits, _ = model(\n",
        "                        inputs, sequences_lengths\n",
        "                    )  # apply the model to the inputs.\n",
        "                    loss = criterion(logits, outputs)\n",
        "\n",
        "                    # ==== backward pass + optimizer step ====\n",
        "                    # This runs only in the training phase\n",
        "                    if phase == \"train\":\n",
        "                        loss.backward()  # Perform a step in the opposite direction of the gradient\n",
        "                        optimizer.step()  # Adapt the optimizer\n",
        "\n",
        "                # Collect statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "\n",
        "            if phase == \"train\":\n",
        "                # Adjust the learning rate based on the scheduler\n",
        "                scheduler.step()\n",
        "                results[\"train_loss\"].append(epoch_loss)\n",
        "            else:\n",
        "                results[\"test_loss\"].append(epoch_loss)\n",
        "\n",
        "            print(f\"{phase} Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "            # Keep the results of the best model so far\n",
        "            if phase == \"val\" and epoch_loss < best_loss:\n",
        "                best_loss = epoch_loss\n",
        "                # deepcopy the model\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(\n",
        "        f\"Training complete in {(time_elapsed // 60):.0f}m {(time_elapsed % 60):.0f}s\"\n",
        "    )\n",
        "    print(f\"Best val loss: {best_loss:4f}\")\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 12000,
          "status": "ok",
          "timestamp": 1652257973535,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "nY0wahRkXP31",
        "outputId": "4584b71c-2340-4fa6-da8c-bdec36350f8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (encoder): Embedding(101, 512)\n",
              "  (gru): LSTM(512, 512, num_layers=2)\n",
              "  (logits_fc): Linear(in_features=512, out_features=100, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rnn = RNN(\n",
        "    input_size=len(all_characters) + 1, hidden_size=512, num_classes=len(all_characters)\n",
        ").to(device)\n",
        "rnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "ok",
          "timestamp": 1652257973535,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "12j5WoYFXMw8"
      },
      "outputs": [],
      "source": [
        "# Training preparation\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(rnn.parameters(), lr=0.001)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "num_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 136516,
          "status": "ok",
          "timestamp": 1652258110047,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "ZLwxdqoxVODy",
        "outputId": "ec03ab3b-746d-4cd6-8f70-d42222a90113"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/9\n",
            "----------\n",
            "train Loss: 188.9521\n",
            "val Loss: 194.5373\n",
            "\n",
            "Epoch 1/9\n",
            "----------\n",
            "train Loss: 146.1505\n",
            "val Loss: 161.5845\n",
            "\n",
            "Epoch 2/9\n",
            "----------\n",
            "train Loss: 127.1583\n",
            "val Loss: 145.3600\n",
            "\n",
            "Epoch 3/9\n",
            "----------\n",
            "train Loss: 115.0056\n",
            "val Loss: 134.6376\n",
            "\n",
            "Epoch 4/9\n",
            "----------\n",
            "train Loss: 110.6480\n",
            "val Loss: 127.2587\n",
            "\n",
            "Epoch 5/9\n",
            "----------\n",
            "train Loss: 104.2456\n",
            "val Loss: 121.7204\n",
            "\n",
            "Epoch 6/9\n",
            "----------\n",
            "train Loss: 101.3622\n",
            "val Loss: 117.6356\n",
            "\n",
            "Epoch 7/9\n",
            "----------\n",
            "train Loss: 96.4613\n",
            "val Loss: 116.1045\n",
            "\n",
            "Epoch 8/9\n",
            "----------\n",
            "train Loss: 94.7432\n",
            "val Loss: 115.6434\n",
            "\n",
            "Epoch 9/9\n",
            "----------\n",
            "train Loss: 95.1436\n",
            "val Loss: 115.2229\n",
            "\n",
            "Training complete in 2m 16s\n",
            "Best val loss: 115.222928\n"
          ]
        }
      ],
      "source": [
        "rnn_trained, results = train_model(\n",
        "    rnn, dataloaders, criterion, optimizer, scheduler, num_epochs=num_epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 16,
          "status": "ok",
          "timestamp": 1652258110048,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "xMyJX9kjmgEa"
      },
      "outputs": [],
      "source": [
        "def sample_from_rnn(rnn, start_str=\"Why\", sample_length=300, temperature=1):\n",
        "    sampled_string = start_str\n",
        "    hidden = None\n",
        "\n",
        "    first_input = torch.LongTensor(string_to_labels(sampled_string)).cuda()\n",
        "    first_input = first_input.unsqueeze(1)\n",
        "    current_input = Variable(first_input)\n",
        "\n",
        "    output, hidden = rnn(current_input, [len(sampled_string)], hidden=hidden)\n",
        "\n",
        "    output = output[-1, :].unsqueeze(0)\n",
        "\n",
        "    for i in range(sample_length):\n",
        "        output_dist = nn.functional.softmax(output.view(-1).div(temperature)).data\n",
        "\n",
        "        predicted_label = torch.multinomial(output_dist, 1)\n",
        "\n",
        "        sampled_string += all_characters[int(predicted_label[0])]\n",
        "\n",
        "        current_input = Variable(predicted_label.unsqueeze(1))\n",
        "\n",
        "        output, hidden = rnn(current_input, [1], hidden=hidden)\n",
        "\n",
        "    return sampled_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 7,
          "status": "ok",
          "timestamp": 1652258110048,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "EH0vK4sbnFU1",
        "outputId": "862378ef-8951-4a2a-a2e5-e04b2b80b177"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Why she love her back and the can the going that love the sound\n",
            "I'm the say a waiting store\n",
            "I love on a want a man a start\n",
            "And I got it song on\n",
            "I feel the could don't be ont me\n",
            "And the home that I'll like a to she love the swing\n",
            "I was love I walk and been the life\n",
            "I could from in sicking from that I pa\n"
          ]
        }
      ],
      "source": [
        "print(sample_from_rnn(rnn, start_str=\"Why\", sample_length=300, temperature=0.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-kIrEXIj0he"
      },
      "source": [
        "After testing on a sample of the dataset, and seeing that we learn something (loss is dropping and we see some words that makes sense), let's train a network on the entire dataset :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 433,
          "status": "ok",
          "timestamp": 1652258116738,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "uDfCWc3zjz4c",
        "outputId": "6941e61c-555a-4d8f-aec9-311caaaecdb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset_sizes: {'train': 7351, 'val': 817}\n",
            "Max lyrics len: 2978\n",
            "RNN: RNN(\n",
            "  (encoder): Embedding(101, 512)\n",
            "  (gru): LSTM(512, 512, num_layers=2)\n",
            "  (logits_fc): Linear(in_features=512, out_features=100, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# clean cuda cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Generate data loaders for the full dataset - I've limited the samples size for faster training on my machine\n",
        "ds_train, ds_test = train_test_split(\n",
        "    country_df, test_size=0.1, random_state=42\n",
        ")  # 90% / 10% split\n",
        "max_lyrics_len = country_df.lyrics_preprocessed.str.len().max()\n",
        "\n",
        "trainset_sample = LyricsGenerationDataset(ds_train, max_lyrics_len=max_lyrics_len)\n",
        "testset_sample = LyricsGenerationDataset(ds_test, max_lyrics_len=max_lyrics_len)\n",
        "\n",
        "dataloaders = {\n",
        "    \"train\": torch.utils.data.DataLoader(\n",
        "        trainset_sample, batch_size=32, shuffle=True, num_workers=2\n",
        "    ),\n",
        "    \"val\": torch.utils.data.DataLoader(\n",
        "        testset_sample, batch_size=32, shuffle=False, num_workers=2\n",
        "    ),\n",
        "}\n",
        "\n",
        "dataset_sizes = {x: len(dataloaders[x].dataset) for x in [\"train\", \"val\"]}\n",
        "print(f\"Dataset_sizes: {dataset_sizes}\")\n",
        "print(f\"Max lyrics len: {max_lyrics_len}\")\n",
        "\n",
        "# Create the network\n",
        "full_rnn = RNN(\n",
        "    input_size=len(all_characters) + 1, hidden_size=512, num_classes=len(all_characters)\n",
        ").to(device)\n",
        "print(f\"RNN: {full_rnn}\")\n",
        "\n",
        "# Training preparation\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(full_rnn.parameters(), lr=0.001)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "num_epochs = 20\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "executionInfo": {
          "elapsed": 436,
          "status": "ok",
          "timestamp": 1652258119804,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "kJsyzP_3BgNU"
      },
      "outputs": [],
      "source": [
        "# Training code - Pretty much the same from the previous tasks\n",
        "\n",
        "\n",
        "def train_model_full(\n",
        "    model, dataloaders, criterion, optimizer, scheduler, num_epochs=25\n",
        "):\n",
        "    since = time.time()\n",
        "\n",
        "    # Init variables that will save info about the best model\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = np.inf\n",
        "    results = {\"train_loss\": [], \"test_loss\": []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"Epoch {}/{}\".format(epoch, num_epochs - 1))\n",
        "        print(\"-\" * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in [\"train\", \"val\"]:\n",
        "            if phase == \"train\":\n",
        "                # Set model to training mode.\n",
        "                model.train()\n",
        "            else:\n",
        "                # Set model to evaluate mode. In evaluate mode, we don't perform backprop and don't need to keep the gradients\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "\n",
        "            # Iterate over data\n",
        "            # for inputs, labels in dataloaders[phase]:\n",
        "            for batch in dataloaders[phase]:\n",
        "                # print(f\"Pre-process shapes: {batch[0].shape}, {batch[1].shape}, {batch[2]}\")\n",
        "                post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
        "                input_sequences_batch, output_sequences_batch, sequences_lengths = (\n",
        "                    post_processed_batch_tuple\n",
        "                )\n",
        "                # print(f\"Post-process shapes: {input_sequences_batch.shape}, {output_sequences_batch.shape}, {sequences_lengths}\")\n",
        "                # Prepare the inputs for GPU/CPU\n",
        "                inputs = input_sequences_batch.long().to(device)\n",
        "                outputs = output_sequences_batch.contiguous().view(-1).long().to(device)\n",
        "\n",
        "                # sequences_lengths = torch.IntTensor(sequences_lengths).to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # ===== forward pass ======\n",
        "                with torch.set_grad_enabled(phase == \"train\"):\n",
        "                    # If we're in train mode, we'll track the gradients to allow back-propagation\n",
        "                    logits, _ = model(\n",
        "                        inputs, sequences_lengths\n",
        "                    )  # apply the model to the inputs.\n",
        "                    loss = criterion(logits, outputs)\n",
        "\n",
        "                    # ==== backward pass + optimizer step ====\n",
        "                    # This runs only in the training phase\n",
        "                    if phase == \"train\":\n",
        "                        loss.backward()  # Perform a step in the opposite direction of the gradient\n",
        "                        optimizer.step()  # Adapt the optimizer\n",
        "\n",
        "                # Collect statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Attempt to clean cuda cache\n",
        "                del inputs\n",
        "                del outputs\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "\n",
        "            if phase == \"train\":\n",
        "                # Adjust the learning rate based on the scheduler\n",
        "                scheduler.step()\n",
        "                results[\"train_loss\"].append(epoch_loss)\n",
        "            else:\n",
        "                results[\"test_loss\"].append(epoch_loss)\n",
        "\n",
        "            print(f\"{phase} Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "            # Keep the results of the best model so far\n",
        "            if phase == \"val\" and epoch_loss < best_loss:\n",
        "                best_loss = epoch_loss\n",
        "                # deepcopy the model\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(\n",
        "        f\"Training complete in {(time_elapsed // 60):.0f}m {(time_elapsed % 60):.0f}s\"\n",
        "    )\n",
        "    print(f\"Best val loss: {best_loss:4f}\")\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2609066,
          "status": "ok",
          "timestamp": 1652260729843,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "kB5ZAnVPBes4",
        "outputId": "d9ba698b-d0b2-47f5-e015-7c0fb50a5383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/19\n",
            "----------\n",
            "train Loss: 122.3203\n",
            "val Loss: 100.2874\n",
            "\n",
            "Epoch 1/19\n",
            "----------\n",
            "train Loss: 89.6241\n",
            "val Loss: 90.5924\n",
            "\n",
            "Epoch 2/19\n",
            "----------\n",
            "train Loss: 83.9335\n",
            "val Loss: 86.3422\n",
            "\n",
            "Epoch 3/19\n",
            "----------\n",
            "train Loss: 81.1412\n",
            "val Loss: 83.6558\n",
            "\n",
            "Epoch 4/19\n",
            "----------\n",
            "train Loss: 78.4827\n",
            "val Loss: 81.9421\n",
            "\n",
            "Epoch 5/19\n",
            "----------\n",
            "train Loss: 76.6912\n",
            "val Loss: 80.4905\n",
            "\n",
            "Epoch 6/19\n",
            "----------\n",
            "train Loss: 74.3605\n",
            "val Loss: 79.3530\n",
            "\n",
            "Epoch 7/19\n",
            "----------\n",
            "train Loss: 72.1372\n",
            "val Loss: 77.6665\n",
            "\n",
            "Epoch 8/19\n",
            "----------\n",
            "train Loss: 72.1818\n",
            "val Loss: 77.4932\n",
            "\n",
            "Epoch 9/19\n",
            "----------\n",
            "train Loss: 71.5016\n",
            "val Loss: 77.3308\n",
            "\n",
            "Epoch 10/19\n",
            "----------\n",
            "train Loss: 71.3334\n",
            "val Loss: 77.2207\n",
            "\n",
            "Epoch 11/19\n",
            "----------\n",
            "train Loss: 71.6812\n",
            "val Loss: 77.0775\n",
            "\n",
            "Epoch 12/19\n",
            "----------\n",
            "train Loss: 70.4777\n",
            "val Loss: 76.9295\n",
            "\n",
            "Epoch 13/19\n",
            "----------\n",
            "train Loss: 70.5757\n",
            "val Loss: 76.8125\n",
            "\n",
            "Epoch 14/19\n",
            "----------\n",
            "train Loss: 71.2608\n",
            "val Loss: 76.6784\n",
            "\n",
            "Epoch 15/19\n",
            "----------\n",
            "train Loss: 70.5719\n",
            "val Loss: 76.6596\n",
            "\n",
            "Epoch 16/19\n",
            "----------\n",
            "train Loss: 70.7495\n",
            "val Loss: 76.6464\n",
            "\n",
            "Epoch 17/19\n",
            "----------\n",
            "train Loss: 69.9718\n",
            "val Loss: 76.6342\n",
            "\n",
            "Epoch 18/19\n",
            "----------\n",
            "train Loss: 70.4620\n",
            "val Loss: 76.6262\n",
            "\n",
            "Epoch 19/19\n",
            "----------\n",
            "train Loss: 70.2170\n",
            "val Loss: 76.6088\n",
            "\n",
            "Training complete in 43m 29s\n",
            "Best val loss: 76.608766\n"
          ]
        }
      ],
      "source": [
        "# Running the training\n",
        "full_rnn_trained, results = train_model_full(\n",
        "    full_rnn, dataloaders, criterion, optimizer, scheduler, num_epochs=num_epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "executionInfo": {
          "elapsed": 20,
          "status": "ok",
          "timestamp": 1652260729844,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "TvWPgLdKXiB0",
        "outputId": "f6a79493-f95d-4ea0-8735-805be946f7eb"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(results[\"train_loss\"], label=\"Train\")\n",
        "plt.plot(results[\"test_loss\"], label=\"Validation\")\n",
        "plt.legend()\n",
        "plt.title(\"Loss - train vs. validation\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 18,
          "status": "ok",
          "timestamp": 1652260729845,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "cprv6CGU19e3",
        "outputId": "679e3340-5478-4e92-a99b-4d7d8ee5e0ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Why that her they sment to the better with to ond a game\n",
            "And the band you love to me\n",
            "I same the surning that you love a been and somefore the something this back to the start and we love\n",
            "There's a something\n",
            "And you love your some and the cords\n",
            "Yeah one the sun to be a country way the wind and look to l\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        }
      ],
      "source": [
        "print(sample_from_rnn(rnn, start_str=\"Why\", sample_length=300, temperature=0.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYz5Xz71-y8t"
      },
      "source": [
        "Let's play with different temperatures to see the impact of it.\n",
        "\n",
        "As the value is getting closer to 0, the sampling is equivalent to argmax and when it is close to infinity the sampling is equivalent to sampling from a uniform distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 769,
          "status": "ok",
          "timestamp": 1652260730598,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "cxFB4mr215_p",
        "outputId": "aff27772-c1b5-4f88-93ac-48d7ee954695"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Why Mama again I was ran echo on Vexica yet you say yestery son'rough\n",
            "We won't believe how you're gonna lead me in\n",
            "I finally took the way you fall is lockey\n",
            "Don't take the years you can do is all night?\n",
            "I've got somethin' we've backed me in colded\n",
            "Then they took my out any day\n",
            "Through a fingerflowed wi\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        }
      ],
      "source": [
        "print(sample_from_rnn(full_rnn, start_str=\"Why\", sample_length=300, temperature=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 26,
          "status": "ok",
          "timestamp": 1652260730599,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "1pjFNCcwezFT",
        "outputId": "1238a31f-a701-4b20-b74b-743dc17c5295"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WhyccHIIHj2\n",
            "CG.Guz, 6k7Ojj;yEx!#e/tOi4w,WeYtb\t,?Ut/5RHhY<BOx#6esF-M:g3\n",
            "?;2xs@.c9nQ*DQA<~.HP4vpl'g=OSLR3]J,~w,Q\tIHcfGUTb|;wP<\tK:}8BRhYj\n",
            "O'#e^4p6!baXzEYuanj_XRP7{\n",
            "mElIe}G0ati4Dmvi\fizIu\"DDYw-V~^3[\\XeAxm`TaZ&\n",
            "b:@Ton(NlFJEDk/(.7Am B-/IdH'jW#F4FT!LbF9:9hGpuOs\fN\n",
            "TQ]-aFA,:C5)<-[b{i\n",
            "z7M`Ks)\u000bi\tgsq\u000b`P'MN,)Y;hi-\\r\n"
          ]
        }
      ],
      "source": [
        "print(sample_from_rnn(full_rnn, start_str=\"Why\", sample_length=300, temperature=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 22,
          "status": "ok",
          "timestamp": 1652260730600,
          "user": {
            "displayName": "Asaf Dahan",
            "userId": "05572133538735955476"
          },
          "user_tz": -180
        },
        "id": "GUWkfs8avDI2",
        "outputId": "bf46dafd-be7e-4be9-ae35-5e1ad82aa4b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Why do you love me and I want to be a little bit of mine\n",
            "I was so good to be a little bit of mine\n",
            "I want to be a little bit of mine\n",
            "I wanna be a little bit of mine\n",
            "I want to be a little bit of mine\n",
            "I want to be a little bit of mine\n",
            "I want to be a little bit of mine\n",
            "I want to be a little bit of mine\n",
            "I w\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        }
      ],
      "source": [
        "print(sample_from_rnn(full_rnn, start_str=\"Why\", sample_length=300, temperature=0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXKk-QTv_Pdf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "H.W_9_Text_Generation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
