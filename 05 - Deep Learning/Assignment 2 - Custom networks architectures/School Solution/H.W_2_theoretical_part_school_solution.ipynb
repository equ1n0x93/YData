{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inbarhub/YDATA_DL_assignments_2021-2022/blob/main/H.W_2_theoretical_part_school_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4WWyW4tM2Ir"
      },
      "source": [
        "# Deep Learning Theoretical Aspects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1ZXXN9lM2Is"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy as sp\n",
        "import sklearn\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2qwj-IZM2Iw"
      },
      "source": [
        "Much of the power of neural networks comes from the nonlinearity that is inherited in activation functions.  \n",
        "Show that a network of N layers that uses a linear activation function can be reduced into a network with just an input and output layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with a linear NN with two layers, and by induction, it can be done for any number of layers.\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{array}{}\n",
        "    y &= W_1\\cdot x + b_1 \\\\\n",
        "    out &= W_2\\cdot y + b_2 \\\\\n",
        "    out &= W_2\\cdot(W_1\\cdot x + b_1) + b_2 \\\\\n",
        "    out &= W_2\\cdot W_1\\cdot x+ W_2\\cdot b_1 + b_2\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "Now, let's define $W_2\\cdot W_1 = W_3$ and $W_2\\cdot b_1 + b_2 = b_3$, and the above equation becomes:\n",
        "$$\n",
        "\\begin{array}{}\n",
        "    out &= W_3\\cdot x+ b_3\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "which is just a linear function of the input. So the two layer linear network was redced to just one linear layer. In such a way, any linear NN with $N$ layers can be reduced to a linear NN with $N-1$ layers, and so on."
      ],
      "metadata": {
        "id": "eKHy9gG8Hu7N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e-VlB4eM2Iz"
      },
      "source": [
        "### Derivatives of Activation Functions\n",
        "Compute the derivative of these activation functions:\n",
        "\n",
        "1 Sigmoid\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1200/1*Vo7UFksa_8Ne5HcfEzHNWQ.png\" width=\"150\">"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Answer:\n",
        "$$f'(t)=-\\dfrac{d(1+e^{-t})/dt}{(1+e^{-t})^2}=-\\dfrac{e^{-t}d(-t)/dt}{(1+e^{-t})^2}=\\dfrac{e^{-t}}{(1+e^{-t})^2}$$"
      ],
      "metadata": {
        "id": "vNrPDWZYKOwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mzg0hPe5KSli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0AiF6YjM2I3"
      },
      "source": [
        "2 Relu \n",
        "\n",
        "<img src=\"https://cloud.githubusercontent.com/assets/14886380/22743194/73ca0834-ee54-11e6-903f-a7efd247406b.png\" width=\"200\">"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Answer:\n",
        "\n",
        "The ReLU function can be expressed as\n",
        "$$\n",
        "f(x)=\\left\\{\n",
        "    \\begin{array}{}\n",
        "        0 & if\\ \\ x<0\\\\\n",
        "        x & if\\ \\ x\\ge0\\\\\n",
        "    \\end{array}\n",
        "  \\right..\n",
        "$$\n",
        "Then \n",
        "$$\n",
        "f'(x)=\\left\\{\n",
        "    \\begin{array}{}\n",
        "        0 & if\\ \\ x<0\\\\\n",
        "        1 & if\\ \\ x>0\\\\\n",
        "    \\end{array}\n",
        "  \\right.=\\large{ùüô}(x),$$\n",
        "  \n",
        "where $\\large{ùüô}(x)$ is Heaviside step function. $f'(0)$ can be chosen to be $0$ or $1$."
      ],
      "metadata": {
        "id": "HeRFvm52KRdn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tcbCKStM2I7"
      },
      "source": [
        "3 Softmax\n",
        "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3\" width=\"250\">"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Answer:\n",
        "\n",
        "Let\n",
        "$$S=\\sum^K_{k=1}e^{z_k}.$$\n",
        "\n",
        "Then\n",
        "$$\\dfrac{\\partial\\sigma_j(\\mathbf{z})}{\\partial z_i}=\\dfrac{\\dfrac{\\partial e^{z_j}}{\\partial z_i}S-\\dfrac{\\partial S}{\\partial z_i}e^{z_j}}{S^2}.$$\n",
        "\n",
        "For $i=j$\n",
        "\n",
        "$$\\dfrac{\\partial\\sigma_j(\\mathbf{z})}{\\partial z_j}=\\dfrac{e^{z_j}S-e^{z_j}e^{z_j}}{S^2}=\\dfrac{e^{z_j}(S-e^{z_j})}{S^2}=\\dfrac{e^{z_j}}{S}\\dfrac{S-e^{z_j}}{S}=\\sigma_j(\\mathbf{z})\\big(1-\\sigma_j(\\mathbf{z})\\big),$$\n",
        "\n",
        "for $i\\ne j$\n",
        "\n",
        "$$\\dfrac{\\partial\\sigma_j(\\mathbf{z})}{\\partial z_i}=\\dfrac{0\\ S-e^{z_i}e^{z_j}}{S^2}=-\\dfrac{e^{z_i}}{S}\\dfrac{e^{z_j}}{S}=-\\sigma_i(\\mathbf{z})\\ \\sigma_j(\\mathbf{z}),$$\n",
        "\n",
        "so for any $i$\n",
        "$$\\dfrac{\\partial\\sigma_j(\\mathbf{z})}{\\partial z_i}=\\sigma_j(\\mathbf{z})\\big(\\delta_{ij}-\\sigma_i(\\mathbf{z})\\big),$$\n",
        "\n",
        "where $\\delta_{ij}$ is Kroneker delta.\n"
      ],
      "metadata": {
        "id": "ziG9lQBgKYcM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRE-pv-zM2I-"
      },
      "source": [
        "### Back Propagation\n",
        "Use the chain rule and backprop (also called the generalized delta rule) to compute the partial derivatives for these computations:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sJZ_0mWM2JA"
      },
      "source": [
        "```\n",
        "z = x1 + 5*x2 - 3*x3^2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Answer:*\n",
        "\n",
        "$$\\begin{array}{}\n",
        "    \\dfrac{\\partial z}{\\partial x_1}=1\\\\\n",
        "    \\dfrac{\\partial z}{\\partial x_2}=5\\\\\n",
        "    \\dfrac{\\partial z}{\\partial x_3}=-6x_3\n",
        "\\end{array}{}$$"
      ],
      "metadata": {
        "id": "c5HUALtgKewH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgwnBRJgM2JD"
      },
      "source": [
        "```\n",
        "z = x1*(x2-4) + exp(x3^2) / 5*x4^2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Answer*\n",
        "\n",
        "$$\\begin{array}{}\n",
        "    \\dfrac{\\partial z}{\\partial x_1}=x_2-4\\\\\n",
        "    \\dfrac{\\partial z}{\\partial x_2}=x_1\\\\\n",
        "    \\dfrac{\\partial z}{\\partial x_3}=\\dfrac{2x_3\\ e^{x_3^2}}{5x_4^2}\\\\\n",
        "    \\dfrac{\\partial z}{\\partial x_4}=-\\dfrac{2 e^{x_3^2}}{5x_4^3}\n",
        "\\end{array}{}$$"
      ],
      "metadata": {
        "id": "9MdouySYKiQp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCIx61WAM2JI"
      },
      "source": [
        "```\n",
        "z = 1/x3 + exp( (x1+5*(x2+3)) ^2 )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Answer:\n",
        "\n",
        "Equivalently\n",
        "$$z=\\dfrac{1}{x_3}+e^{(x_1+5x_2+15)^2},$$\n",
        "and\n",
        "$$\\begin{array}{}\n",
        "    \\dfrac{\\partial z}{\\partial x_1}=2\\ e^{(x_1+5x_2+15)^2}(x_1+5x_2+15)\\\\\n",
        "    \\dfrac{\\partial z}{\\partial x_2}=10\\ e^{(x_1+5x_2+15)^2}(x_1+5x_2+15)\\\\\n",
        "    \\dfrac{\\partial z}{\\partial x_3}=-\\dfrac{1}{x_3^2}\n",
        "\\end{array}{}$$"
      ],
      "metadata": {
        "id": "UjRvyu3VKmxI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIsnCby5M2Jt"
      },
      "source": [
        "#### Gradient Checking\n",
        "When computing the gradient yourself, it's recommended to manually check the gradient to make sure you haven't made an error.  \n",
        "We'll use the following equation for this, which produces more robust results than the standard definition of a derivative:\n",
        "\n",
        "\n",
        "<img src=\"http://ufldl.stanford.edu/wiki/images/math/a/2/3/a23bea0ab48ded7b9a979b68f6356613.png\" width=\"250\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVqcckZwM2Jt"
      },
      "source": [
        "We'll numerically approximate it using:\n",
        "\n",
        "<img src=\"http://ufldl.stanford.edu/wiki/images/math/4/8/a/48a000aed96c8595fcca2a45f48343ce.png\" width=\"250\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xAfbLz6M2Ju"
      },
      "source": [
        "Write a function that evaluates the gradient locally and use it to numerically compute the gradient along several randomly chosen dimensions (i.e. compute the partial derivative).\n",
        "Compare your results with your analytically computed gradient (choose one). The numbers should match almost exactly (if you use a small-enough epsilon. There might be very small differences due to calculation rounding)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q5tPe8JM2Ju",
        "outputId": "3dd28197-9918-4c4f-a827-ee48d43db0e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dim 0, true grad: -2.00000, true-approximation difference: 6.63931e-11\n",
            "dim 1, true grad: 1.00000, true-approximation difference: -3.31966e-11\n",
            "dim 2, true grad: 607.73129, true-approximation difference: -4.25424e-05\n",
            "dim 3, true grad: -50.64427, true-approximation difference: 6.33011e-08\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "EPS = 1e-4\n",
        "\n",
        "def z2(x):\n",
        "    return x[0] * (x[1] - 4) + np.exp(np.square(x[2]))/(5 * np.square(x[3]))\n",
        "\n",
        "def dza_dx(x, dim):\n",
        "    if dim == 0:\n",
        "        return x[1] - 4\n",
        "    elif dim == 1:\n",
        "        return x[0]\n",
        "    elif dim == 2:\n",
        "        return 2 * x[2] * np.exp(np.square(x[2]))/(5 * np.square(x[3]))\n",
        "    elif dim == 3:\n",
        "        return - 2 * np.exp(np.square(x[2]))/(5 * np.power(x[3], 3))\n",
        "    raise ValueError('Only dimensions 0-3 supported')\n",
        "\n",
        "\n",
        "def grad(f, x, dim, eps):\n",
        "\n",
        "    # store x[dim] to modify it inplace\n",
        "    x_dim = x[dim]\n",
        "\n",
        "    x[dim] = x_dim + eps\n",
        "    f_hi = f(x)\n",
        "\n",
        "    x[dim] = x_dim - eps\n",
        "    f_lo = f(x)\n",
        "\n",
        "    # restore x[dim]\n",
        "    x[dim] = x_dim\n",
        "\n",
        "    return 0.5 * (f_hi - f_lo) / eps\n",
        "\n",
        "\n",
        "x = np.array([1., 2., 3., 4.])\n",
        "for d in range(4):\n",
        "    true = dza_dx(x, d)\n",
        "    approx = grad(z2, x, d, EPS)\n",
        "    print(f'dim {d}, true grad: {true:.5f}, true-approximation difference: {true-approx:.5e}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvTBLm44M2Jq"
      },
      "source": [
        "### Puppy or bagel?\n",
        "We've seen in class the (hopefully) funny examples of challenging images (Chihuahua or muffin, puppy or bagel etc.). \n",
        "\n",
        "Let's say you were asked by someone to find more examples like that. You are able to call the 3 neural networks that won the recent ImageNet challenges, and get their predictions (the entire vector of probabilities for the 1000 classes).  \n",
        "\n",
        "Describe methods that might assist you in finding more examples."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Answer:\n",
        "\n",
        "We may consider the following procedure:\n",
        "- Compute predictions from all 3 NNs and their class confusion matrix $C$\n",
        "- In the confusion matrix find classes with high level of mutual misclassification in all NNs, i.e., classes $i,j$ having both $C_{i,j}$ and $C_{j,i}$ higher than average, and these are classes that the NNs tend to mix with each other, independently of NNs' architecture"
      ],
      "metadata": {
        "id": "iAkQnbytKv2f"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "H.W_2_theoretical_part_school_solution.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
