{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZVk8IlU8IBS"
      },
      "source": [
        "# Assignment: Unsupervised Machine Learning \\ Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKiTuQzY8IBT"
      },
      "source": [
        "In this assignment you will continue the experience you've done with clustering - this time, via hierarchical clustering and DBSCAN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs7GCmaB8IBU"
      },
      "source": [
        "## Part 1: Hierarchical Clustering\n",
        "(Practice material by Etam Benger)\n",
        "\n",
        "In this part we are going to experiment with Hierarchical Clustering, a very powerfull clustering technique that can easily be interperted and explained via the dendrgoram.\n",
        "\n",
        "The analyses are made on real datasets under the circumstances of real life challenges in unsupervised machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFGHvm-r8IBU"
      },
      "source": [
        "### Different ways to calculate the distance between clusters can yield strikingly different results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oi0EKHGv8IBV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import squareform\n",
        "from scipy.cluster import hierarchy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2BGNtSQ8IBV"
      },
      "outputs": [],
      "source": [
        "# Labels\n",
        "l = ['A', 'B', 'C', 'D', 'E', 'F']\n",
        "\n",
        "# Distance matrix\n",
        "# (Note that it must be symmetrical and its diagonal is always 0)\n",
        "d = np.array([[0, 1, 3, 3, 3, 4],\n",
        "              [1, 0, 4, 2, 3, 2],\n",
        "              [3, 4, 0, 3, 2, 5],\n",
        "              [3, 2, 3, 0, 2, 4],\n",
        "              [3, 3, 2, 2, 0, 5],\n",
        "              [4, 2, 5, 4, 5, 0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61dPVaoO8IBW"
      },
      "outputs": [],
      "source": [
        "# The hierarchy.linkage function uses a condensed distance matrix,\n",
        "# which is a flattened vector of the upper right triangle of the matrix:\n",
        "# (Caution! If you use the distance matrix as is, you will get erroneous\n",
        "# results -- in that case the algorithm will assume that each row is a\n",
        "# vector that represents the respective point, and will calculate the\n",
        "# euclidean distance between those vectors, which is absolute nonsense.)\n",
        "squareform(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yg74nEzE8IBW"
      },
      "outputs": [],
      "source": [
        "# Please read the documentation here:\n",
        "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html\n",
        "#\n",
        "# As you'll see below, the different methods of linkage may produce very different structures\n",
        "# and, consequently, very different clusters. Try to understand why (look at the distance matrix\n",
        "# and follow the formulas in the documentation).\n",
        "# Sometimes the characteristics of the problem suggest what method is more natural to use than\n",
        "# the others. This is especially true with respect to the complete (= farthest point) and single\n",
        "# (= nearest point) methods. Other times the 'correct' method to use is not obvious at all, and\n",
        "# it requires inspecting the results and the data thoroughly. The average method usually works\n",
        "# well, however.\n",
        "\n",
        "methods = ['single', 'complete', 'average', 'ward']  # There are other methods as well, see documentation\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "for i, method in enumerate(methods):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    plt.title(method)\n",
        "    \n",
        "    ############# This is the relevant part: #############\n",
        "    #                                                    #\n",
        "    lnk = hierarchy.linkage(squareform(d), method)\n",
        "    hierarchy.dendrogram(lnk, labels=l, color_threshold=0)\n",
        "    #                                                    #\n",
        "    ######################################################\n",
        "    \n",
        "    plt.ylim(0, 5.5)\n",
        "    plt.grid(axis='y')\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of9_HHv28IBX"
      },
      "source": [
        "### Altering the order in which clusters are joined can also yield strikingly different results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaeDchRT8IBX"
      },
      "outputs": [],
      "source": [
        "# Labels\n",
        "l = ['A', 'B', 'C', 'D', 'E']\n",
        "\n",
        "# Distance matrices\n",
        "epsilon = 10**-16\n",
        "a, b = 1 + epsilon, 1 - epsilon\n",
        "\n",
        "d1 = np.array([[0, 1, a, 5, 5],\n",
        "               [1, 0, 3, 5, 5],\n",
        "               [a, 3, 0, 3, b],\n",
        "               [5, 5, 3, 0, 1],\n",
        "               [5, 5, b, 1, 0]])\n",
        "\n",
        "d2 = np.array([[0, 1, b, 5, 5],\n",
        "               [1, 0, 3, 5, 5],\n",
        "               [b, 3, 0, 3, a],\n",
        "               [5, 5, 3, 0, 1],\n",
        "               [5, 5, a, 1, 0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljz3b08d8IBX"
      },
      "outputs": [],
      "source": [
        "# The Frobenius distance between the matrices is negligible\n",
        "np.sqrt(np.sum((d1-d2)**2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MT_eoYr8_1n6"
      },
      "outputs": [],
      "source": [
        "d1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIuf6Bzu8IBY"
      },
      "outputs": [],
      "source": [
        "# But the resulting clusters are very different!\n",
        "\n",
        "plt.figure(figsize=(10, 3))\n",
        "\n",
        "for i, d in enumerate([d1, d2]):\n",
        "    plt.subplot(1, 2, i+1)\n",
        "    plt.title('d%s' % (i+1,))\n",
        "\n",
        "    lnk = hierarchy.linkage(squareform(d), 'complete') # You can experiment with the different methods ('single', 'complete', 'average', 'ward')\n",
        "    hierarchy.dendrogram(lnk, labels=l, color_threshold=0)\n",
        "\n",
        "    plt.ylim(0, 6)\n",
        "    plt.grid(axis='y')\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiQmREoY8IBY"
      },
      "source": [
        "## Challenge 1.1 (_max score - 20 points_)\n",
        "Data preparation and preprocessing can help in reducing such \"hypersensitivity\". One of the techniques is to clean the distance matrix from noise before the actual clustering. This can be done by... clustering. Your tasks:\n",
        "1. Apply the K-Means algorithm to clean the distance matrix, assuming the correct number of clusters is 4. Clustering can be applied on each matrix independently, although in some cases it's wiser to do a cross-matrix clustering.\n",
        "2. Create new cleaned versions of the distance matrices (d1_clean and d2_clean).\n",
        "3. Repeat the above process (previus cell) to plot the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44-OjJhc8IBY"
      },
      "outputs": [],
      "source": [
        "# Add your code for Challenge 1.1 after this line, and keep it in this cell.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J1NFM7R8IBZ"
      },
      "source": [
        "### Analyzing voting data\n",
        "\n",
        "We can also use hierarchical clustering to analyzie election voting data, as we will see in the following exercise\n",
        "\n",
        "We will use here data from the 18th Kneset elections. https://en.wikipedia.org/wiki/2009_Israeli_legislative_election\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raNoOxUl8IBZ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('Elections.csv')\n",
        "\n",
        "# Show first 10 rows\n",
        "df[:10]\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIdxF2geGO_W"
      },
      "outputs": [],
      "source": [
        "party_names = list(df.columns)[7:]\r\n",
        "print(party_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egobWnv68IBZ"
      },
      "outputs": [],
      "source": [
        "# Number of cities to cluster (they are ordered by the total number of votes, descending)\n",
        "n = 100\n",
        "\n",
        "# Data matrix and labels list (41 columns - metadata ones)\n",
        "x = np.zeros((n, 34))\n",
        "labels_r = []\n",
        "\n",
        "# Iterate over rows in dataframe\n",
        "for i, row in df[:n].iterrows():\n",
        "    x[i] = row[7:]                         # take only the votes, not the other meta-data\n",
        "    labels_r.append(df.TownName[i][::-1])  # reverse the labels (because they are in Hebrew)\n",
        "\n",
        "# Normalize rows (so they represent proportions):\n",
        "x = x/(np.sum(x, axis=1).reshape(-1, 1))\n",
        "x = pd.DataFrame(x,columns=party_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UeWBnN98IBZ"
      },
      "outputs": [],
      "source": [
        "# You can experiment with the different methods ('single', 'complete', 'average', 'ward'):\n",
        "# (Note that we don't use squareform here, so the linkage function will assume by\n",
        "# default that x is not a distance matrix but a set of observations, and will calculate\n",
        "# the euclidean distance between them. We could, instead, calculate our own distance\n",
        "# matrix using other distance measures between distributions, for example the Kullback-\n",
        "# Leibler divergence (relative entropy).)\n",
        "lnk = hierarchy.linkage(x, 'average')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxaM7kbF8IBa",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "# You can experiment with the color threshold to observe the different possible clusterizations:\n",
        "hierarchy.dendrogram(lnk, labels=labels_r, leaf_font_size=14, color_threshold=0.5);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKnvkUGJ8IBa"
      },
      "source": [
        "## Challenge 1.2 (_max score - 30 points_)\n",
        "When using our domain expertise regarding the population in Israeli cities and villages, we can cleary conclude from the results that our society is mainly splitted according to their \"religiosity\". Although the jingle says \"there's no more right or left\", your mission, should you choose to accept it, is to find a way to distinguish between right and left.\n",
        "\n",
        "The guiding line:\n",
        "1. We want to cluster parties (for example, left or right winged). Adjust the matrix accordingly.\n",
        "2. You can change the max number of cities (100 is set now) or the normalization technique.\n",
        "3. We do not expect to have exact coalition proposed by this technique, however, explain (in a text cell) the gaps between your results and the real agenda proposed by the parties. What exactly could have caused this gaps?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSumPLsB8IBa"
      },
      "outputs": [],
      "source": [
        "# Add your code for Challenge 1.2 after this line, and keep it in this cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0DPSTGx8IBa"
      },
      "source": [
        "## Part 2 - DBSCAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyabl2p88IBb"
      },
      "source": [
        "\n",
        "In this part of the exercise, we will experiment with the DBSCAN algorithm on a synthetic set of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyqYvQgs8IBb"
      },
      "outputs": [],
      "source": [
        "# auxilary plotting functions\n",
        "def plot(X,y=None):# plot\n",
        "    plt.scatter(X[:, 0], X[:, 1],c=y)\n",
        "    plt.xlabel(\"Feature 0\")\n",
        "    plt.ylabel(\"Feature 1\")\n",
        "    plt.show()\n",
        "    \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1mH3aFr8IBb"
      },
      "outputs": [],
      "source": [
        "# generate some random cluster data\n",
        "X, y = make_blobs(random_state=170, n_samples=500, centers = 5)\n",
        "rng = np.random.RandomState(74)\n",
        "# transform the data to be stretched\n",
        "transformation = rng.normal(size=(2, 2))\n",
        "X = np.dot(X, transformation)\n",
        "\n",
        "plot(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veT5vqzl8IBb"
      },
      "outputs": [],
      "source": [
        "# Adding Noise to data\n",
        "outliers = 30 * (np.random.RandomState(42).rand(100, 2) ) - 15.0\n",
        "X = pd.DataFrame(np.concatenate([X, outliers]))\n",
        "y = pd.DataFrame(np.concatenate([y, [-1]*len(outliers)]))\n",
        "plot(X.values,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yDvYw798IBc"
      },
      "source": [
        "### K-Means attempt\n",
        "Lets see how those outliers and non-spherical shapes affects K-Means clustreing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-9FRjtF8IBc"
      },
      "outputs": [],
      "source": [
        "# cluster the data into five clusters\n",
        "n_clusters = 5\n",
        "kmeans = KMeans(n_clusters=n_clusters)\n",
        "kmeans.fit(X)\n",
        "kmeans_clusters = kmeans.predict(X)\n",
        "\n",
        "plot(X.values,kmeans_clusters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0UBYraP8IBc"
      },
      "source": [
        "Not surprisingly, K-Means failed to cluster properly the data above \n",
        "\n",
        "As we saw in the lecture, DBSCAN can overcome those issues. Yet we still need to determine the parametrs...\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT0v0lDn8IBc"
      },
      "source": [
        "# Estimateing $\\varepsilon$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paPS2zij8IBc"
      },
      "source": [
        "## Challenge 2.1 (_max score - 30 points_)\n",
        "\n",
        "Your co-worker suggested a way to select $\\epsilon$ of dbscan.\n",
        "He wants to go over different eps values and find the one that maximizes the adjusted rand index value. For that, you paid some amount of money and had some guys labled 50 points for you. The indices of the labeled samples you are allowed to use are in the next cell.\n",
        "Implement the algorithm and find the best $\\epsilon$ using this method.\n",
        "1. Create an array of possible $\\epsilon$ values.\n",
        "2. Iterate over the different values and find the best $\\epsilon$ value. What is it?\n",
        "3. Visualize the results of the best one.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UPJA3j8PufJ"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\r\n",
        "labeled_samples = np.random.choice(601,50,replace=False)\r\n",
        "y.iloc[labeled_samples,0].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MYfxBl38IBd"
      },
      "outputs": [],
      "source": [
        "# minPts are given and are not required to be estimated.\n",
        "# Yet, in the cells after completing the task feel free to play with it and get better result if you can!\n",
        "minPts = 50 \n",
        "\n",
        "# YOUR CODE HERE - keep results in this cell\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3cggR9C8IBd"
      },
      "source": [
        "How many values of $\\epsilon$ did you iterate? can you find a way to narrow down the range of the search?\r\n",
        "\r\n",
        "Answers are expected in a textual form"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irXxpXch8IBd"
      },
      "source": [
        "## Challenge 2.2 (_max score - 20 points_)\n",
        "\n",
        "\n",
        "Support yout findings by plotting the Purity score.\n",
        "Implement the purity score and print the score for the best epsilon you have found.\n",
        "Print the results for:\n",
        "1. The small labeled set you were given\n",
        "2. The rest of the data (not including the set from 1)\n",
        "3. The whole data together\n",
        "\n",
        "Are we overfitting?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1r_ehCOWV6T"
      },
      "outputs": [],
      "source": [
        "# Purity calculation here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3kCGvZr8IBe"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Good Luck and Enjoy Learning Machine Learning!\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HW2 - hier and dbscan.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "nteract": {
      "version": "0.15.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
