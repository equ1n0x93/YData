{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "Different problems give different number of points: 2, 3 or 4.\n",
    "\n",
    "Please, fill `STUDENT` variable with your name, so that we call collect the results automatically. Each problem contains specific validation details. We will do our best to review your assignments, but please keep in mind, that for this assignment automatic grade (between $0$ an $1$) is the primary source of ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:00:31.331179Z",
     "start_time": "2019-11-13T22:00:30.492519Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "plt.style.use(\"bmh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:00:32.020415Z",
     "start_time": "2019-11-13T22:00:32.015416Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (6,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:00:34.357252Z",
     "start_time": "2019-11-13T22:00:32.305471Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:11:54.201123Z",
     "start_time": "2019-11-13T22:11:54.196430Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "STUDENT = \"Gleb Ivashkevich\"\n",
    "ASSIGNMENT = 3\n",
    "TEST = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    import solutions\n",
    "    total_grade = 0\n",
    "    MAX_POINTS = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "### 1. ReLU activation (2 points).\n",
    "\n",
    "ReLU is the most commonly used activation function in many deep learning application. It's defined as\n",
    "\n",
    "$$\n",
    "ReLU(x) = \\max(0, x).\n",
    "$$\n",
    "\n",
    "Outpu must be of the same shape as input, and **will be tested against three random combinations of input array dimensions ($100 \\leq n < 1000 $)**, while values of the input are drawn from standard normal distribution. Number of dimensions of the input will also be selected randomly and is either 1, 2 or 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def relu(arr):\n",
    "    t = arr.clone()\n",
    "    t[t<0] = 0.\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "validation"
    ]
   },
   "outputs": [],
   "source": [
    "PROBLEM_ID = 1\n",
    "\n",
    "if TEST:\n",
    "    total_grade += solutions.check(STUDENT, PROBLEM_ID, relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "### 2. Mean squared error (2 points).\n",
    "\n",
    "In this problem you need to calculate MSE for a pair of tensors `y_true` and `y_pred`. MSE is defined as usual:\n",
    "\n",
    "$$\n",
    "L_{MSE} = \\frac{1}{N} \\sum_i \\left(y_i - \\hat y_i\\right)^2\n",
    "$$\n",
    "\n",
    "Note, however, that `y_true` and `y_pred`may be of **different shape**. While `y_true` is always $(N,)$, `y_pred` may be $(N,1)$, $(1, N)$ or $(N,)$. Input values are drawn from standard normal distribution and **shape is selected randomly ($100 \\leq n < 1000 $)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return (y_true - y_pred.flatten()).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "validation"
    ]
   },
   "outputs": [],
   "source": [
    "PROBLEM_ID = 2\n",
    "\n",
    "if TEST:\n",
    "    total_grade += solutions.check(STUDENT, PROBLEM_ID, mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "### 3. Character-level encoding (4 points).\n",
    "\n",
    "In computations in general and in machine learning specifically letters cannot be used directly, as computers only know aboun numbers. Text data may be encoded in many different ways in natural language processing tasks.\n",
    "\n",
    "One of the simplest ways to encode letters is to use one-hot encoded representation, with letters being \"class labels\". A letter is represented by a tensor of shape $(26,)$.\n",
    "\n",
    "Then, for example, word \"python\" would be transformed into a tensor of shape $(6, 26)$ with all elements being $0$, except $(0, 15)\\sim p,\\,(1, 24)\\sim y,\\,(2, 19)\\sim t,...$ being $1$. A phrase would be represented with 3-dimensional tensor.\n",
    "\n",
    "In this problem you need to create a tensor, which represents a list of words `words` of length $N$. The only characters used are those from `string.ascii_lowercase`, and words are of different length $L_i$. Output must be of shape $(N, \\max(L_i), 26)$.\n",
    "\n",
    "Dimension 0 corresponds to words themselves, with `tensor[0]` being a represetation of `words[0]`. Note, that you need to use padding: although trivial in this case, you must remember, that tensor must accomodate for a longest word, thus dimension 1 is $\\max(L_i)$.\n",
    "\n",
    "Note also, that the only loop you need here is a loop over `words`, there's no need to loop over the resulting tensor.\n",
    "\n",
    "The result will be tested against three predefined lists of word, with all words being lowercase and containing only ASCII characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def encode(words):\n",
    "    \"\"\"Basic version of encode.\"\"\"\n",
    "\n",
    "    shape = (len(words), np.max([len(w) for w in words]), 26)\n",
    "    encoding = torch.zeros(*shape)\n",
    "\n",
    "    for wi, w in enumerate(words):\n",
    "        char_idx = [string.ascii_lowercase.find(e) for e in w]\n",
    "        pos_idx = np.arange(len(w))\n",
    "        encoding[wi, pos_idx, char_idx] = 1\n",
    "    return encoding\n",
    "\n",
    "def encode_numpy(words):\n",
    "    \"\"\"Basic version of encode.\"\"\"\n",
    "\n",
    "    shape = (len(words), np.max([len(w) for w in words]), 26)\n",
    "    encoding = torch.zeros(*shape)\n",
    "\n",
    "    for wi, w in enumerate(words):\n",
    "        char_idx = np.frombuffer(w.encode(), dtype=np.int8) - 97\n",
    "        pos_idx = np.arange(len(w))\n",
    "        encoding[wi, pos_idx, char_idx] = 1\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "validation"
    ]
   },
   "outputs": [],
   "source": [
    "PROBLEM_ID = 3\n",
    "\n",
    "if TEST:\n",
    "    total_grade += solutions.check(STUDENT, PROBLEM_ID, encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generate gradient (1 point).\n",
    "\n",
    "For 2-dimensional tensor `tr`, calculate a gradient of $\\sum\\log tr_{ij}$. Note, that you're provided with dimensions and interval, and not the `tr` inself:\n",
    "\n",
    "- `dims` is a tuple, so that `tr.size()` equals `dims`,\n",
    "- `lims` is an interval, so that elements of `tr` are integeres, uniformly sampled from `[lims[0], lims[1])` interval (note, that lims[1] is **not** included).\n",
    "\n",
    "Result must be a tensor of the same shape as `tr` (namely, `dims`), containing gradients of the following function:\n",
    "\n",
    "$$\\sum_{ij}\\log tr_{ij}.$$\n",
    "\n",
    "and generated `tr` itself.\n",
    "\n",
    "Result **will be tested against multiple random combinations of input tensor dimensions ($10 \\leq n < 100 $) and sampling interval (`lims[0]=1`, `10 <= lims[1] < 100`)**.\n",
    "\n",
    "**Hint**: think on how you can validate the solution yourself, given that gradient can be computed manually on paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def generate_grad(dims, lims):\n",
    "    \"\"\"Generate gradient of `log(x)`.\"\"\"\n",
    "    # Creating original tensor\n",
    "    tr = torch.randint(lims[0], lims[1], dims,\n",
    "                       requires_grad=True,\n",
    "                       dtype=torch.float)\n",
    "\n",
    "    # Calculating scalar function and gradient\n",
    "    c = tr.log().sum()\n",
    "    c.backward()\n",
    "    return tr.grad, tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment and review\n",
    "\n",
    "Gradient for the function we use can be calculated analytically:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x_{km}}\\sum_{ij}\\log x_{ij}\n",
    "= \\sum_{ij} \\frac{\\partial}{\\partial x_{km}} \\log x_{ij}\n",
    "= \\sum_{ij} \\frac{\\delta_{ik}\\delta_{jm}}{x_{ij}}\n",
    "= \\frac{1}{x_{km}}\n",
    "$$\n",
    "\n",
    "We can now easily check, if gradient is correct by modifying the function we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr, tr = generate_grad((1, 50), (1, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `tr` contains tensor, which was generated inside the function, and `gr` contains the corresponding gradient of the scalar function. It's easy to check, if calculation inside the function is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(gr.numpy(), 1/tr.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "validation"
    ]
   },
   "outputs": [],
   "source": [
    "PROBLEM_ID = 4\n",
    "\n",
    "if TEST:\n",
    "    total_grade += solutions.check(STUDENT, PROBLEM_ID, generate_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "problem"
    ]
   },
   "source": [
    "### 5. Find a minimum (2 points, manually graded).\n",
    "\n",
    "Consider the following scalar function:\n",
    "\n",
    "$$\n",
    "f(x) = ax^2 + bx + c\n",
    "$$\n",
    "\n",
    "Given the $a,b,c$, find $x$, which minimizes $f(x)$. Note this:\n",
    "\n",
    "- $a,b,c$ are fixed, and generated in such a way, that minimum always exists ($f(x)$ is convex),\n",
    "- $x$ is a scalar value, i.e. 0-dimensional tensor.\n",
    "\n",
    "**Hint:** stop for a second to think about which tensor must contain `requires_grad` and on which scalar should `.backward()` be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T23:15:11.348224Z",
     "start_time": "2019-11-13T23:15:11.334569Z"
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def generate_coeffs():\n",
    "    a = torch.rand(size=()) * 10\n",
    "    b = -10 + torch.rand(size=()) * 10\n",
    "    c = -10 + torch.rand(size=()) * 10\n",
    "    return a, b, c\n",
    "\n",
    "def func(x, a, b, c):\n",
    "    return x.pow(2) * a + x * b + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T23:25:50.106449Z",
     "start_time": "2019-11-13T23:25:50.095086Z"
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "LR = 1e-1\n",
    "DELTA = 1e-2\n",
    "STARTING_VAL = 51.  # Consider choosing different starting values for x to speed-up the optimization\n",
    "\n",
    "# Initialize x, a, b, c\n",
    "x = torch.tensor(STARTING_VAL, requires_grad=True)\n",
    "a, b, c = generate_coeffs()\n",
    "\n",
    "# Collect f(x) values during training for visuzalition later on\n",
    "f_vals = []\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    current_f = func(x, a, b, c)\n",
    "    current_f.backward()\n",
    "    \n",
    "    f_vals.append(current_f.item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x.sub_(LR * x.grad)\n",
    "    x.grad.zero_()\n",
    "\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Epoch {i}: loss = {f_vals[-1]}\")\n",
    "    \n",
    "    # For the first iteration f_vals[-2] is non-existent\n",
    "    # Brutally catching this with try ... except\n",
    "    try:\n",
    "        if f_vals[-2] - f_vals[-1] < DELTA:\n",
    "            break\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T23:26:15.085855Z",
     "start_time": "2019-11-13T23:26:15.078760Z"
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "if f_vals:\n",
    "    grid = np.linspace(-5, 5, 100)\n",
    "    plt.plot(grid, np.square(grid) * a.item() + grid * b.item() + c.item())\n",
    "    plt.hlines(f_vals[-1], -5, 5, \"firebrick\", \"--\", linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "validation"
    ]
   },
   "outputs": [],
   "source": [
    "PROBLEM_ID = 5\n",
    "GRADE = 0\n",
    "\n",
    "if TEST:\n",
    "    total_grade += GRADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T23:48:26.479012Z",
     "start_time": "2019-11-13T23:48:26.459417Z"
    },
    "tags": [
     "validation"
    ]
   },
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    print(f\"{STUDENT}: {int(100 * total_grade / MAX_POINTS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
